{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIS.py \n",
      "Main program\n",
      "Working directory: C:\\Users\\kibob\\TEst\\P7G10\n",
      "g10 TTACTGCCCTGTGGGGCAAG\n",
      "Expected WT distance: 96\n",
      "Program Running\n",
      "HBBCD34patient07G10_R1_001.fastq: Total_reads:39481, [('g10', 27232)]\n",
      "HBBCD34patient07WT_R1_001.fastq: Total_reads:43918, [('g10', 43741)]\n",
      "SUMMARY\n",
      "Total wells with product: 2\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import csv\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import pandas as pd \n",
    "from itertools import izip\n",
    "\n",
    "\n",
    "#CRIS.py\n",
    "\n",
    "#Modify parameters below in the get_parameters() section.\n",
    "\n",
    "def get_parameters():\n",
    "    #Note to user- Change text inside of quote marks ('YOUR DNA SEQUENCES GO HERE') for your experiment.  Case of text does not matter.\n",
    "    ID = 'P7G10'\n",
    "    ref_seq = str.upper('tacggctgtcatcacttagacctcaccctgtggagccacaccctagggttggccaatctactcccaggagcagggagggcaggagccagggctgggcataaaagtcagggcagagccatctattgcttacatttgcttctgacacaactgtgttcactagcaacctcaaacagacaccatggtgcatctgactcctgaggagaagtctgccgttactgccctgtggggcaaggtgaacgtggatgaagttggtggtgaggccctgggcaggttggtatcaaggttacaagacaggtttaaggagaccaatagaaactgggcatgtggagacagagaagactcttgggtttctgataggcactgactctctctgcctattggtctattttcccacccttaggctgctggtggtctacccttggacccagaggttctttgagtcctttggggatctgtccactcctgatgctgttatgggcaaccctaaggt')\n",
    "    seq_start = str.upper('caactgtgttcacta')\n",
    "    seq_end = str.upper('gggcaaggtgaacgt')\n",
    "    fastq_files = '*.fastq'\n",
    "    test_list = [\n",
    "               str('g10'),   str.upper('ttactgccctgtggggcaag'), \n",
    "                ]\n",
    "\n",
    "    return ID,ref_seq,seq_start,seq_end,fastq_files,test_list\n",
    "\n",
    "\n",
    "def pairwise(iterable):\n",
    "    #Make an ordered dictionary from items in in test list\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return izip(a, a)\n",
    "\n",
    "\n",
    "def make_project_directory(save_dir):\n",
    "    #Create a project directory to store files in\n",
    "    cwd = os.getcwd()   #Get current working directory\n",
    "    try:\n",
    "        os.stat(save_dir)\n",
    "    except:\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "def write_to_file(record_entry,f):\n",
    "    #Writes results to the results_counter.txt file.  Method of inserting space between each well to make the .txt file easier to read.  Based on # of items in well list\n",
    "    f.write(\"\\n\")\n",
    "    for record in record_entry:         #record entry is a giant list (master_Record) that contains well-lists as elements\n",
    "        for line in record:\n",
    "            if len(record) > 2:\n",
    "                f.write(str(line) + '\\n')\n",
    "            else:\n",
    "                pass\n",
    "        if len(line) > 2:\n",
    "            f.write('\\n\\n') \n",
    "        else:\n",
    "            pass        \n",
    "\n",
    "def make_counter(indel_size_list, current_fastq_file, dict_Counters, c_Counter, SNP_test, raw_wt_counter):\n",
    "    top_common = 12             #Number of top found reads to write to results_counter .txt file\n",
    "    temp_counter = Counter(indel_size_list).most_common(top_common)    #Count top indels present in fastq file\n",
    "    temp_dict =OrderedDict()                                           \n",
    "    #If there are not a total of at least 'top_common' common reads from the summary file, fill the spaces with NA, NA\n",
    "    if len(Counter(indel_size_list).most_common(top_common)) <top_common:\n",
    "            for i in range (0,top_common - len(Counter(indel_size_list).most_common(top_common))):\n",
    "                temp_counter.append((\"NA\",\"NA\"))\n",
    "            else:\n",
    "                pass\n",
    "    temp_dict['Name']=str(current_fastq_file)              #Fill in Name column of CSV file with the file fastq name\n",
    "    temp_dict['Sample']=''                                 #Makes a blank column for user to fill in sample names\n",
    "    counter =1\n",
    "    \n",
    "    for k in dict_Counters:\n",
    "        try:\n",
    "            temp_dict[k] = str( str(dict_Counters[k]) + ' (' + str(format((dict_Counters[k] / c_Counter*100), '.1f')) + '%)')\n",
    "            temp_dict[str('%'+ k)] = str(format((dict_Counters[k] / c_Counter)*100, '.1f'))\n",
    "            temp_dict['Total'] = c_Counter\n",
    "            temp_dict['SNP_test'] = SNP_test\n",
    "            temp_dict['raw_wt_counter'] = raw_wt_counter\n",
    "            temp_dict['Total_indel'] = str((format(((c_Counter - indel_size_list.count(0))/c_Counter)*100,'.1f')))\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "        \n",
    "    for k,g in temp_counter:        #Fill in top indels sizes and amount\n",
    "        temp_dict['#'+ str(counter)+ '-Indel']=k\n",
    "        try:\n",
    "          # temp_dict['z%Indel'+str(counter)]= format(g/c_Counter *100, '.1f')\n",
    "           temp_dict['#'+ str(counter)+'-Reads(%)'] = str(g) + ' ('+ str(format(g/c_Counter*100, '.1f')) + '%)'\n",
    "        except TypeError:\n",
    "            pass\n",
    "        counter+=1\n",
    "    return temp_dict\n",
    "    \n",
    "\n",
    "\n",
    "def search_fastq(ID,ref_seq,seq_start,seq_end,fastq_files,test_list):\n",
    "    #Process the fastq file and look for reads\n",
    "    test_dict=OrderedDict()                  #Name, Sequence of each item that is being searched for           \n",
    "    dict_Counters = OrderedDict()            #Name, Count of each item in the dictionary being searched for (ie current_fastq_file)\n",
    "    master_distance_and_count_summary =[]\n",
    "    save_dir = os.getcwd()+\"/\"+ str(ID) + \"/\"   \n",
    "    fastq_counter = 0               #Count the number of fastq_files with reads\n",
    "    master_Record = []             #Master list, Master list contains lists\n",
    "    indel_size_list = []             #list of indel sizes found in the fastq.  For each read, one indel size is recorded\n",
    "    csv_summary_df = pd.DataFrame()\n",
    "    master_distance_and_count_summary =[]\n",
    "\n",
    "    for x,y in pairwise(test_list):          #Create an ordered dictionary of items in test_list\n",
    "        test_dict[x] = y\n",
    "    save_dir = os.getcwd()+\"/\"+ str(ID) + \"/\"\n",
    "    print \"Working directory: {}\".format(str(os.getcwd()))#save_dir\n",
    "    make_project_directory(save_dir)\n",
    "    file_name = save_dir+'results_counter ' + ID + '.txt'\n",
    "    f = open(file_name, \"w\")\n",
    "    wt_distance = ref_seq.find(seq_end)+len(seq_end) - ref_seq.find(seq_start)      #Expected size of WT read, the distance between the two anchor points made from seq_start and seq_end\n",
    "    f.write(ID + '\\n')\n",
    "    f.write(str(\"seq_start: \"+seq_start+'\\n'))    \n",
    "    f.write(str(\"seq_end: \"+seq_end+'\\n'))\n",
    "    f.write(\"Test_Sequences: \\n\")\n",
    "    for key, value in test_dict.iteritems():                #Go through the test_dict and write each item that is being searched for\n",
    "        f.write(str(key)+\": \"+value+'\\n')\n",
    "        print key, value\n",
    "        dict_Counters[str(key)]=0\n",
    "        \n",
    "    print 'Expected WT distance: {}'.format(wt_distance)     #The expected distance between  seq_start and seq_end if the DNA is WT/ REF\n",
    "    if wt_distance < 0:\n",
    "        f.write(\"\\n\\n WARNING: THIS IS NOT GOING TO GIVE YOU THE FULL DATA. YOUR EXPECTED WT DISTANCE IS LESS THAN 0, it is: {}\\n  Check your seq_start and seq_end again\\n\".format(wt_distance))\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print \"Program Running\"\n",
    "\n",
    "    for each_fastq_file in glob.glob(fastq_files):   #For each clone (really each fastq file in directory), open the file as \"clone\"\n",
    "        c_Counter = 0                     #Reset control counter to 0, this counter counts how many times both seq_start and seq_end are found in a line.\n",
    "        start_counter = 0                  #How many times seq_start is found in a fastq file, used for SNP check\n",
    "        end_counter = 0                    #How many times seq_end is found in a fastq file, used for SNP check\n",
    "        raw_wt_counter=0                   #Calculate how many times first item in test_list is found in fastqfile, a check for SNPs\n",
    "        for items in test_dict:            # RESET DICT COUNTERS TO 0\n",
    "            dict_Counters[str(items)]=0\n",
    "        indel_size_list = []                          #List of all the indel sizes found in each file\n",
    "        fastq_name = str(each_fastq_file)                                #Well name that contains the clone being screened\n",
    "        line_list =[]                                  #List of all the lines found in a fastq file.  These lines must contain both seq_start and seq_end.  Used to find most highly occuring sequences\n",
    "        current_fastq_file = open(str(each_fastq_file), \"r\")     \n",
    "        for line in current_fastq_file:   #For each line in the fastq file\n",
    "            if test_dict.items()[0][1] in line:           #Counts the number of times the first item in test_dict is found in ANY of the lines of the fastq file.  This is a check in case SNPs are in BOTH seq_start and seq_end\n",
    "                raw_wt_counter+=1\n",
    "            if line.find(seq_start)>0 and line.find(seq_end)>0:  \n",
    "                c_Counter += 1\n",
    "                start_counter +=1     #Count # of times seq_start is found\n",
    "                end_counter +=1       #Count # of times seq_end is found\n",
    "                read_start = line.find(seq_start)\n",
    "                read_end = line.find(seq_end)+len(seq_end)\n",
    "                indel_size = line.find(seq_end)+len(seq_end) - line.find(seq_start) - wt_distance\n",
    "                indel_size_list.append(indel_size)\n",
    "                line_list.append(line[read_start:(read_end)])\n",
    "                for item in test_dict:\n",
    "                    if test_dict[item] in line:\n",
    "                        dict_Counters[item] +=1\n",
    "                    else:\n",
    "                        pass\n",
    "            elif line.find(seq_start)>0 and line.find(seq_end)<0:        #If seq_start is found and seq_end is not found, for SNPT test\n",
    "                start_counter +=1\n",
    "            elif line.find(seq_end)>0 and line.find(seq_start)<0:        #If seq_end is found and seq_start is not found, for SNP test\n",
    "                end_counter+=1\n",
    "            else:\n",
    "                pass\n",
    "        current_fastq_file.close()               \n",
    "        try:\n",
    "            SNP_test = format(start_counter / end_counter, '.2f')        #Compare counts of seq_start and seq_end for SNP test\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "        try:\n",
    "            raw_wt_counter = str(str(raw_wt_counter) +  ' ('+ format((raw_wt_counter/ dict_Counters[test_dict.items()[0][0]]), '.1f') +')') #Calculate raw_wt_counter\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "       \n",
    "        if c_Counter == 0 :\n",
    "            pass\n",
    "        elif c_Counter > 10 :  #if more than 10 control read counts, record data\n",
    "            print\"{}: Total_reads:{}, {}\".format(fastq_name,str(c_Counter).ljust(2), dict_Counters.items())\n",
    "            fastq_counter += 1\n",
    "            test_list_string=str(\" Testing: \")\n",
    "            for k,v in dict_Counters.items():\n",
    "                test_list_string=test_list_string+\"({}:{}), \".format(k,v)\n",
    "            temp = Counter(line_list).most_common(12)\n",
    "            #summary_line is a list, format:  Miller-Plate13-C01 TOTAL:2072 OrderedDict([('g3', 2010), ('Block_Only', 0), ('Mod_Only', 2), ('Block_Mod', 0), ('Full', 0)])       [(0, 2070), (-1, 2)]\n",
    "            summary_line = ([str(fastq_name) + \" TOTAL:\" + str(c_Counter)+\" \"+test_list_string+\"     \"+\"Top_reads:\"+ str(Counter(indel_size_list).most_common(12))])\n",
    "            for k,v in temp:          #Append the top found DNA sequences to summary_line\n",
    "                summary_line.append('{} , {}'.format(k,v))\n",
    "            master_Record.append(summary_line)\n",
    "            master_distance_and_count_summary.append(make_counter(indel_size_list,str(fastq_name), dict_Counters, c_Counter,SNP_test,raw_wt_counter))\n",
    "        else:\n",
    "             pass\n",
    "\n",
    "\n",
    "    print\"SUMMARY\"\n",
    "    make_project_directory(ID)\n",
    "    #print master_distance_and_count_summary\n",
    "    pd_columns = ['Name','Sample','Total', 'Total_indel', '#1-Indel','#1-Reads(%)','#2-Indel','#2-Reads(%)','#3-Indel','#3-Reads(%)','#4-Indel','#4-Reads(%)','#5-Indel','#5-Reads(%)',\n",
    "             '#6-Indel','#6-Reads(%)','#7-Indel','#7-Reads(%)','#8-Indel','#8-Reads(%)', 'SNP_test', 'raw_wt_counter']\n",
    "\n",
    "    flip_dict = test_dict.items()   #Need to flip order of items in dictionary.  That way when they are inserted into the excel list, the order will come out correct\n",
    "    flip_dict.reverse()\n",
    "    flip_dict=OrderedDict(flip_dict)\n",
    "    for k, v in flip_dict.iteritems():        #Insert the items from test_dict into position 3 for the column output, after 'Total'\n",
    "        pd_columns.insert(3,k)\n",
    "    for k, v in test_dict.iteritems():        #Insert the % values for test_dic at the end\n",
    "        pd_columns.append(str('%'+k))\n",
    "    csv_summary_df = pd.DataFrame.from_records(master_distance_and_count_summary, index='Name', columns=pd_columns)\n",
    "    csv_summary_df=csv_summary_df.sort_index()\n",
    "    csv_summary_df = csv_summary_df[pd.notnull(csv_summary_df['Total'])]\n",
    "    csv_summary_df.to_csv(str(save_dir+ID)+'.csv')    #Filename to save csv as\n",
    "    master_Record = sorted(master_Record)\n",
    "    print \"Total wells with product:\", fastq_counter\n",
    "    write_to_file(master_Record,f)\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "\n",
    "def main():\n",
    "    ID = ''\n",
    "    ref_seq = ''\n",
    "    seq_start = ''\n",
    "    seq_end = ''\n",
    "    fastq_files = ''\n",
    "    test_list = []\n",
    "    print(\"CRIS.py \\nMain program\")\n",
    "    ID, ref_seq, seq_start, seq_end, fastq_files, test_list = get_parameters()\n",
    "    search_fastq(ID, ref_seq, seq_start, seq_end, fastq_files, test_list)\n",
    "    print(\"Done\")\n",
    "if __name__== \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
